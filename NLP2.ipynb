{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Bag-of-Words (BoW):*\n",
        "\n",
        "**CountVectorizer** converts text documents into a matrix of token counts.\n",
        "**The normalized BoW **uses the normalize function (with L1 norm) so that each document vector sums to 1, giving the relative frequency of each word.\n",
        "TF-IDF:\n",
        "\n",
        "**TfidfVectorizer** computes the term frequency-inverse document frequency matrix, which scales down the impact of frequently occurring words that might be less informative.\n",
        "\n",
        "**Word2Vec:**\n",
        "The text is tokenized using NLTK.\n",
        "The Word2Vec model from Gensim is trained on the tokenized documents, resulting in dense vector representations (embeddings) for each word."
      ],
      "metadata": {
        "id": "FDjOdgEgY8qb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rSrFzvslXw6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db81e17e-d7b2-4b18-d53f-acd9496cbfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bag-of-Words Count Occurrence ===\n",
            "   and  are  brown  dog  dogs  fox  is  jump  jumps  lazy  never  not  over  \\\n",
            "0    0    0      1    1     0    1   0     0      1     1      0    0     1   \n",
            "1    0    0      0    1     0    0   0     1      0     1      1    0     1   \n",
            "2    1    1      1    0     1    1   1     0      0     1      0    1     0   \n",
            "\n",
            "   quick  quickly  the  \n",
            "0      1        0    2  \n",
            "1      0        1    1  \n",
            "2      2        0    0   \n",
            "\n",
            "=== Normalized Count Occurrence (L1 norm) ===\n",
            "   and  are     brown       dog  dogs       fox   is      jump     jumps  \\\n",
            "0  0.0  0.0  0.111111  0.111111   0.0  0.111111  0.0  0.000000  0.111111   \n",
            "1  0.0  0.0  0.000000  0.142857   0.0  0.000000  0.0  0.142857  0.000000   \n",
            "2  0.1  0.1  0.100000  0.000000   0.1  0.100000  0.1  0.000000  0.000000   \n",
            "\n",
            "       lazy     never  not      over     quick   quickly       the  \n",
            "0  0.111111  0.000000  0.0  0.111111  0.111111  0.000000  0.222222  \n",
            "1  0.142857  0.142857  0.0  0.142857  0.000000  0.142857  0.142857  \n",
            "2  0.100000  0.000000  0.1  0.000000  0.200000  0.000000  0.000000   \n",
            "\n",
            "=== TF-IDF Matrix ===\n",
            "        and       are     brown       dog      dogs       fox        is  \\\n",
            "0  0.000000  0.000000  0.297062  0.297062  0.000000  0.297062  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.337295  0.000000  0.000000  0.000000   \n",
            "2  0.336732  0.336732  0.256094  0.000000  0.336732  0.256094  0.336732   \n",
            "\n",
            "       jump   jumps      lazy     never       not      over     quick  \\\n",
            "0  0.000000  0.3906  0.230695  0.000000  0.000000  0.297062  0.297062   \n",
            "1  0.443503  0.0000  0.261940  0.443503  0.000000  0.337295  0.000000   \n",
            "2  0.000000  0.0000  0.198880  0.000000  0.336732  0.000000  0.512187   \n",
            "\n",
            "    quickly       the  \n",
            "0  0.000000  0.594123  \n",
            "1  0.443503  0.337295  \n",
            "2  0.000000  0.000000   \n",
            "\n",
            "=== Tokenized Documents ===\n",
            "Document 1: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "Document 2: ['never', 'jump', 'over', 'the', 'lazy', 'dog', 'quickly', '.']\n",
            "Document 3: ['a', 'fox', 'is', 'quick', 'and', 'brown.lazy', 'dogs', 'are', 'not', 'quick', '.']\n",
            "\n",
            "=== Vocabulary in Word2Vec Model ===\n",
            "['the', '.', 'quick', 'fox', 'over', 'lazy', 'dog', 'brown', 'jumps', 'not', 'are', 'jump', 'quickly', 'a', 'is', 'and', 'brown.lazy', 'dogs', 'never'] \n",
            "\n",
            "=== Word2Vec Embedding for 'quick' ===\n",
            "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Ensures the punkt_tab resource is available\n",
        "\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Never jump over the lazy dog quickly.\",\n",
        "    \"A fox is quick and brown.Lazy dogs are not quick.\"\n",
        "]\n",
        "\n",
        "# Preprocess documents (e.g., converting to lowercase)\n",
        "processed_docs = [doc.lower() for doc in documents]\n",
        "\n",
        "# 1. Bag-of-Words (BoW)\n",
        "\n",
        "\n",
        "# 1a. Count occurrence (raw counts)\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(processed_docs)\n",
        "bow_df = pd.DataFrame(bow_counts.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "print(\"=== Bag-of-Words Count Occurrence ===\")\n",
        "print(bow_df, \"\\n\")\n",
        "\n",
        "# 1b. Normalized count occurrence (L1 normalization: each document's frequencies sum to 1)\n",
        "bow_normalized = normalize(bow_counts, norm='l1', axis=1)\n",
        "bow_normalized_df = pd.DataFrame(bow_normalized.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "print(\"=== Normalized Count Occurrence (L1 norm) ===\")\n",
        "print(bow_normalized_df, \"\\n\")\n",
        "\n",
        "\n",
        "# 2. TF-IDF\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"=== TF-IDF Matrix ===\")\n",
        "print(tfidf_df, \"\\n\")\n",
        "\n",
        "\n",
        "# 3. Word2Vec Embeddings\n",
        "\n",
        "# Tokenize documents into words using NLTK\n",
        "tokenized_docs = [nltk.word_tokenize(doc) for doc in processed_docs]\n",
        "print(\"=== Tokenized Documents ===\")\n",
        "for i, tokens in enumerate(tokenized_docs, 1):\n",
        "    print(f\"Document {i}: {tokens}\")\n",
        "print()\n",
        "\n",
        "# Train a Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Display the vocabulary from the Word2Vec model\n",
        "vocab = list(w2v_model.wv.index_to_key)\n",
        "print(\"=== Vocabulary in Word2Vec Model ===\")\n",
        "print(vocab, \"\\n\")\n",
        "\n",
        "# Example: Get the embedding for the word \"quick\"\n",
        "word = \"quick\"\n",
        "if word in w2v_model.wv:\n",
        "    print(f\"=== Word2Vec Embedding for '{word}' ===\")\n",
        "    print(w2v_model.wv[word])\n",
        "else:\n",
        "    print(f\"Word '{word}' not found in the vocabulary.\")\n"
      ]
    }
  ]
}