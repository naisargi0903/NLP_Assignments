{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**FeedForward Network**: A simple feed-forward network applied after the multi-head attention mechanism.\n",
        "\n",
        "**Scaled Dot-Product Attention**: The core attention mechanism that computes attention scores using the query, key, and value.\n",
        "\n",
        "**Multi-Head Attention**: Multi-head attention mechanism where we have multiple attention heads running in parallel.\n",
        "\n",
        "**Positional Encoding**: Since Transformers donâ€™t have any built-in recurrence or convolution, we add positional encodings to the input embeddings to give the model some sense of word order.\n",
        "\n",
        "**Encoder and Decoder Layers**: The encoder and decoder layers implement the attention mechanisms and feed-forward networks as described in the Transformer paper.\n",
        "\n",
        "**Transformer**: The model itself consists of several encoder layers, decoder layers, and a final linear layer to produce outputs.\n",
        "\n",
        "*Usage:*\n",
        "\n",
        "This model can be used for tasks such as machine translation, text generation, and more.\n",
        "The vocab_size and sequence lengths are set as placeholders. You can adjust them based on your dataset."
      ],
      "metadata": {
        "id": "qVxSbWImkCUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fxdVXs8ja7_",
        "outputId": "da843b0a-c28b-47a4-b57f-0ce374fcdf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 10000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Position-wise Feed Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(score, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        return torch.matmul(attention, value)\n",
        "\n",
        "# Multi-Head Attention Mechanism\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0  # Make sure d_model is divisible by num_heads\n",
        "\n",
        "        self.d_k = d_model // num_heads  # Dimension of each head\n",
        "        self.num_heads = num_heads\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections for each head\n",
        "        query = self.query(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply attention to each head\n",
        "        attention = self.attn(query, key, value, mask)\n",
        "\n",
        "        # Concat heads and apply final linear layer\n",
        "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        output = self.out(attention)\n",
        "        return output\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attn(x, x, x, mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.layernorm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.attn1 = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.attn2 = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
        "        attn_output1 = self.attn1(x, x, x, tgt_mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output1))\n",
        "        attn_output2 = self.attn2(x, memory, memory, memory_mask)\n",
        "        x = self.layernorm2(x + self.dropout(attn_output2))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.layernorm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, vocab_size, max_len=5000, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(src.size(-1))  # Scaled embedding\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        tgt = self.embedding(tgt) * math.sqrt(tgt.size(-1))  # Scaled embedding\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "\n",
        "        # Encoder pass\n",
        "        memory = src\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_mask)\n",
        "\n",
        "        # Decoder pass\n",
        "        output = tgt\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(output, memory, tgt_mask, memory_mask)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "# Example of creating a Transformer model\n",
        "model = Transformer(\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    d_ff=2048,\n",
        "    vocab_size=10000,  # Example vocab size\n",
        ")\n",
        "\n",
        "# Example input\n",
        "src = torch.randint(0, 10000, (32, 10))  # Batch of 32, source sequence length 10\n",
        "tgt = torch.randint(0, 10000, (32, 10))  # Batch of 32, target sequence length 10\n",
        "\n",
        "# Forward pass\n",
        "output = model(src, tgt)\n",
        "print(output.shape)  # (batch_size, target_seq_len, vocab_size)\n",
        "\n",
        "#Output:\n",
        "#For each of the 32 sequences in the batch, for each position in the 10-token target sequence,\n",
        "#the output gives a vector of size vocab_size (10000 in this case).\n",
        "#This vector represents the model's prediction for the likelihood of each word in the vocabulary\n",
        "#being the correct token at that position in the sequence.\n"
      ]
    }
  ]
}