{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Using NLTK for tokenization, stemming, and lemmatization.*\n",
        "\n",
        "**Tokenization:** Whitespace, Punctuation-based, Treebank, Tweet, and Multi-Word Expression (MWE).\n",
        "\n",
        "**Stemming:** Porter Stemmer and Snowball Stemmer.\n",
        "\n",
        "**Lemmatization:** WordNet Lemmatizer.**bold text**"
      ],
      "metadata": {
        "id": "4FG9qzR-VnBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZNP-d6iSsoG",
        "outputId": "28fe645b-58ae-4737-ce7c-f1a4ca36e849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.tokenize.mwe import MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")  # Download WordNet data for lemmatization\n",
        "\n",
        "# Sample Text\n",
        "text = \"The quick brown fox, named Mr. Fox, jumps over the lazy dog! #AmazingFox\"\n",
        "\n",
        "# Tokenization Techniques\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "word_punct_tokenizer = WordPunctTokenizer()\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "# Multi-word expression tokenizer\n",
        "mwe_tokenizer = MWETokenizer([(\"Mr.\", \"Fox\"), (\"lazy\", \"dog\")])\n",
        "\n",
        "# Apply Tokenization\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "word_punct_tokens = word_punct_tokenizer.tokenize(text)\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "mwe_tokens = mwe_tokenizer.tokenize(treebank_tokens)  # Apply MWE on Treebank tokens\n",
        "\n",
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "porter_stems = [porter.stem(word) for word in treebank_tokens]\n",
        "snowball_stems = [snowball.stem(word) for word in treebank_tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in treebank_tokens]\n",
        "\n",
        "# Display Results\n",
        "print(\"\\nTokenization\")\n",
        "print(\"Whitespace Tokenizer:\", whitespace_tokens)\n",
        "print(\"Punctuation-based Tokenizer:\", word_punct_tokens)\n",
        "print(\"Treebank Tokenizer:\", treebank_tokens)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
        "print(\"Multi-word Expression (MWE) Tokenizer:\", mwe_tokens)\n",
        "\n",
        "print(\"\\nStemming\")\n",
        "print(\"Porter Stemmer:\", porter_stems)\n",
        "print(\"Snowball Stemmer:\", snowball_stems)\n",
        "\n",
        "print(\"\\nLemmatization\")\n",
        "print(\"WordNet Lemmatizer:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7zVM7tVWL1a",
        "outputId": "9e62511a-23a2-463d-aa4b-bbd18fd6bcea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization\n",
            "Whitespace Tokenizer: ['The', 'quick', 'brown', 'fox,', 'named', 'Mr.', 'Fox,', 'jumps', 'over', 'the', 'lazy', 'dog!', '#AmazingFox']\n",
            "Punctuation-based Tokenizer: ['The', 'quick', 'brown', 'fox', ',', 'named', 'Mr', '.', 'Fox', ',', 'jumps', 'over', 'the', 'lazy', 'dog', '!', '#', 'AmazingFox']\n",
            "Treebank Tokenizer: ['The', 'quick', 'brown', 'fox', ',', 'named', 'Mr.', 'Fox', ',', 'jumps', 'over', 'the', 'lazy', 'dog', '!', '#', 'AmazingFox']\n",
            "Tweet Tokenizer: ['The', 'quick', 'brown', 'fox', ',', 'named', 'Mr', '.', 'Fox', ',', 'jumps', 'over', 'the', 'lazy', 'dog', '!', '#AmazingFox']\n",
            "Multi-word Expression (MWE) Tokenizer: ['The', 'quick', 'brown', 'fox', ',', 'named', 'Mr._Fox', ',', 'jumps', 'over', 'the', 'lazy_dog', '!', '#', 'AmazingFox']\n",
            "\n",
            "Stemming\n",
            "Porter Stemmer: ['the', 'quick', 'brown', 'fox', ',', 'name', 'mr.', 'fox', ',', 'jump', 'over', 'the', 'lazi', 'dog', '!', '#', 'amazingfox']\n",
            "Snowball Stemmer: ['the', 'quick', 'brown', 'fox', ',', 'name', 'mr.', 'fox', ',', 'jump', 'over', 'the', 'lazi', 'dog', '!', '#', 'amazingfox']\n",
            "\n",
            "Lemmatization\n",
            "WordNet Lemmatizer: ['The', 'quick', 'brown', 'fox', ',', 'named', 'Mr.', 'Fox', ',', 'jump', 'over', 'the', 'lazy', 'dog', '!', '#', 'AmazingFox']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}